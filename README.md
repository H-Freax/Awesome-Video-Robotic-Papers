# Awesome-Video-Robotic-Papers

This repository compiles a list of papers related to the application of video technology in the field of robotics.  Continual improvements are being made to this repository. If you come across any relevant papers that should be included, please don't hesitate to open an issue.

## Review


## Robot Arm

- Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers
  - Vidhi Jain, Maria Attarian, Nikhil J Joshi, Ayzaan Wahid, Danny Driess, Quan Vuong, Pannag R Sanketi, Pierre Sermanet, Stefan Welker, Christine Chan, Igor Gilitschenski, Yonatan Bisk, Debidatta Dwibedi
  - [Paper](https://arxiv.org/abs/2403.12943)
  - [Website](https://vid2robot.github.io/)

- OpenVLA: An Open-Source Vision-Language-Action Model
  - Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, Chelsea Finn
  - [Paper](https://arxiv.org/abs/2406.09246)
  - [Website](https://openvla.github.io/)
  - [Code](https://github.com/openvla/openvla)

- Video Language Planning
  - Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Brian Ichter, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua B. Tenenbaum, Leslie Kaelbling, Andy Zeng, Jonathan Tompson
  - [Paper](https://arxiv.org/abs/2310.10625)
  - [Website](https://video-language-planning.github.io/)
  - [Code](https://github.com/video-language-planning/vlp_code)

- Manipulate-Anything: Automating Real-World Robots using Vision-Language Models
  - Jiafei Duan, Wentao Yuan, Wilbert Pumacay, Yi Ru Wang, Kiana Ehsani, Dieter Fox, Ranjay Krishna
  - [Paper](https://arxiv.org/pdf/2406.18915)
  - [Website](https://robot-ma.github.io/)

- Dreamitate: Real-World Visuomotor Policy Learning via Video Generation
  - Junbang Liang, Ruoshi Liu, Ege Ozguroglu, Sruthi Sudhakar, Achal Dave, Pavel Tokmakov, Shuran Song, Carl Vondrick
  - [Paper](https://arxiv.org/abs/2406.16862)
  - [Website](https://dreamitate.cs.columbia.edu/)
  - [Code](https://github.com/cvlab-columbia/dreamitate)

- Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation
  - Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, Tao Kong
  - [Paper](https://arxiv.org/abs/2312.13139)
  - [Website](https://gr1-manipulation.github.io/)
  - [Code](https://github.com/bytedance/GR-1)

- Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning
  - Haoran He, Chenjia Bai, Ling Pan, Weinan Zhang, Bin Zhao, Xuelong Li
  - [Paper](https://arxiv.org/abs/2402.14407)
  - [Website](https://video-diff.github.io/)

## SPOT
- Track2Act: Predicting Point Tracks from Internet Videos Enables Diverse Zero-shot Manipulation
  - Homanga Bharadhwaj, Roozbeh Mottaghi*, Abhinav Gupta*, Shubham Tulsiani*
  - [Paper](https://arxiv.org/abs/2405.01527)
  - [Website](https://homangab.github.io/track2act/)
  - [Code](https://github.com/homangab/Track-2-Act/)

## Other Useful Sources

[Awesome-VideoLLM-Papers](https://github.com/yyyujintang/Awesome-VideoLLM-Papers)

[Awesome-LLMs-for-Video-Understanding](https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding)

[VLM-Eval: A General Evaluation on Video Large Language Models](https://github.com/zyayoung/Awesome-Video-LLMs)

[LLMs Meet Multimodal Generation and Editing: A Survey](https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation)

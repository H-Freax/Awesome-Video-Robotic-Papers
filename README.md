# Awesome-Video-Robotic-Papers

This repository compiles a list of papers related to the application of video technology in the field of robotics.  Continual improvements are being made to this repository. If you come across any relevant papers that should be included, please don't hesitate to open an issue.

## Review
Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation [Paper](https://arxiv.org/abs/2402.07127)

## Paperlist

Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers [Paper](https://arxiv.org/abs/2403.12943)

OpenVLA: An Open-Source Vision-Language-Action Model [Paper](https://arxiv.org/abs/2406.09246) [Website](https://openvla.github.io/) [Code](https://github.com/openvla/openvla)

Video Language Planning [Paper](https://arxiv.org/abs/2310.10625)[Website](https://video-language-planning.github.io/) [Code](https://github.com/video-language-planning/vlp_code)
## Other Useful Sources

[Awesome-VideoLLM-Papers](https://github.com/yyyujintang/Awesome-VideoLLM-Papers)

[Awesome-LLMs-for-Video-Understanding](https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding)

[VLM-Eval: A General Evaluation on Video Large Language Models](https://github.com/zyayoung/Awesome-Video-LLMs)

[LLMs Meet Multimodal Generation and Editing: A Survey](https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation)

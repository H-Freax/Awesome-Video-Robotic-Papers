# Awesome-Video-Robotic-Papers

This repository compiles a list of seminal and cutting-edge papers that explore the application of video technology in the field of robotics. Continual improvements are being made to this repository, and contributions are welcome. If you come across any relevant papers that should be included, please don't hesitate to open an issue.

## Table of Contents

1. [Review Papers](#review-papers)
2. [Robot Arm](#robot-arm)
3. [SPOT](#spot)
4. [Other Useful Sources](#other-useful-sources)

## Review Papers
- **Towards Generalist Robot Learning from Internet Video: A Survey**
  - Robert McCarthy, Daniel C.H. Tan, Dominik Schmidt, Fernando Acero, Nathan Herr, Yilun Du, Thomas G. Thuruthel, Zhibin Li
  - [Paper](https://arxiv.org/pdf/2404.19664)

- **Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation**
  - Chrisantus Eze, Christopher Crick
  - [Paper](https://arxiv.org/abs/2402.07127)

## Robot Arm
- **RAM: Retrieval-Based Affordance Transfer for Generalizable Zero-Shot Robotic Manipulation**
  - Yuxuan Kuang*, Junjie Ye*, Haoran Geng*, Jiageng Mao, Congyue Deng, Leonidas Guibas, He Wang, Yue Wang
  - [Paper](https://arxiv.org/abs/2407.04689)
  - [Website](https://yxkryptonite.github.io/RAM/)
  - [Code](https://github.com/yxKryptonite/RAM_code)

- **Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers**
  - Vidhi Jain, Maria Attarian, Nikhil J Joshi, Ayzaan Wahid, Danny Driess, Quan Vuong, Pannag R Sanketi, Pierre Sermanet, Stefan Welker, Christine Chan, Igor Gilitschenski, Yonatan Bisk, Debidatta Dwibedi
  - [Paper](https://arxiv.org/abs/2403.12943)
  - [Website](https://vid2robot.github.io/)

- **OpenVLA: An Open-Source Vision-Language-Action Model**
  - Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, Chelsea Finn
  - [Paper](https://arxiv.org/abs/2406.09246)
  - [Website](https://openvla.github.io/)
  - [Code](https://github.com/openvla/openvla)

- **Video Language Planning**
  - Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Brian Ichter, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua B. Tenenbaum, Leslie Kaelbling, Andy Zeng, Jonathan Tompson
  - [Paper](https://arxiv.org/abs/2310.10625)
  - [Website](https://video-language-planning.github.io/)
  - [Code](https://github.com/video-language-planning/vlp_code)

- **Manipulate-Anything: Automating Real-World Robots using Vision-Language Models**
  - Jiafei Duan, Wentao Yuan, Wilbert Pumacay, Yi Ru Wang, Kiana Ehsani, Dieter Fox, Ranjay Krishna
  - [Paper](https://arxiv.org/pdf/2406.18915)
  - [Website](https://robot-ma.github.io/)

- **Dreamitate: Real-World Visuomotor Policy Learning via Video Generation**
  - Junbang Liang, Ruoshi Liu, Ege Ozguroglu, Sruthi Sudhakar, Achal Dave, Pavel Tokmakov, Shuran Song, Carl Vondrick
  - [Paper](https://arxiv.org/abs/2406.16862)
  - [Website](https://dreamitate.cs.columbia.edu/)
  - [Code](https://github.com/cvlab-columbia/dreamitate)

- **Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation**
  - Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, Tao Kong
  - [Paper](https://arxiv.org/abs/2312.13139)
  - [Website](https://gr1-manipulation.github.io/)
  - [Code](https://github.com/bytedance/GR-1)

- **Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning**
  - Haoran He, Chenjia Bai, Ling Pan, Weinan Zhang, Bin Zhao, Xuelong Li
  - [Paper](https://arxiv.org/abs/2402.14407)
  - [Website](https://video-diff.github.io/)

- **ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data**
  - Zeyi Liu, Cheng Chi, Eric Cousineau, Naveen Kuppuswamy, Benjamin Burchfiel, Shuran Song
  - [Paper](https://arxiv.org/abs/2406.19464)
  - [Website](https://mani-wav.github.io/)
  - [Code](https://github.com/real-stanford/maniwav)
  - [Dataset](https://real.stanford.edu/maniwav/data/)

- **Vision-based Manipulation from Single Human Video with Open-World Object Graphs**
  - Yifeng Zhu, Arisrei Lim, Peter Stone, Yuke Zhu
  - [Paper](https://arxiv.org/abs/2405.20321)
  - [Website](https://ut-austin-rpl.github.io/ORION-release/)

- **Learning to Act from Actionless Videos through Dense Correspondences**
  - Po-Chen Ko, Jiayuan Mao, Yilun Du, Shao-Hua Sun, Joshua B. Tenenbaum
  - [Paper](https://arxiv.org/abs/2310.08576)
  - [Website](https://flow-diffusion.github.io/)
  - [Code](https://github.com/flow-diffusion/AVDC)

## SPOT
- **Track2Act: Predicting Point Tracks from Internet Videos Enables Diverse Zero-shot Manipulation**
  - Homanga Bharadhwaj, Roozbeh Mottaghi*, Abhinav Gupta*, Shubham Tulsiani*
  - [Paper](https://arxiv.org/abs/2405.01527)
  - [Website](https://homangab.github.io/track2act/)
  - [Code](https://github.com/homangab/Track-2-Act/)

## Other Useful Sources

- [Awesome-VideoLLM-Papers](https://github.com/yyyujintang/Awesome-VideoLLM-Papers)
- [Awesome-LLMs-for-Video-Understanding](https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding)
- [VLM-Eval: A General Evaluation on Video Large Language Models](https://github.com/zyayoung/Awesome-Video-LLMs)
- [LLMs Meet Multimodal Generation and Editing: A Survey](https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation)
